{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnuttaraR/DSGP_Group_03/blob/main/MediaPipe_SSLModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp_qsNAZroAS",
        "outputId": "cd4d69ab-6bd7-4048-a20c-9e2ae46063ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.9.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.8/dist-packages (from mediapipe) (3.19.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mediapipe) (3.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from mediapipe) (1.22.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.8/dist-packages (from mediapipe) (4.6.0.66)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from mediapipe) (23.1.21)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.8/dist-packages (from mediapipe) (22.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (4.38.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (23.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.15.0)\n",
            "Installing collected packages: mediapipe\n",
            "Successfully installed mediapipe-0.9.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BQFwU_ceApgh"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import csv\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from google.colab.patches import cv2_imshow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75L9O6CXql0S"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FDytX1vBu7V"
      },
      "outputs": [],
      "source": [
        "#Running Successfully\n",
        "\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_hands = mp.solutions.hands\n",
        "\n",
        "# RoughSSLDataset - 4 people, 5 words, 5 videos each\n",
        "# C:\\Users\\USER\\Documents\\DEGREE MATERIAL (Year 2)\\CM2603 Data Science Group Project\\RoughSSLDataaset upload this to drive\n",
        "video_dir = \"/content/drive/MyDrive/MediaPipe_Data/Dataset_MediaPipe\"\n",
        "\n",
        "# Initialize MediaPipe Hands model\n",
        "with mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5) as hands:\n",
        "    # Initialize the numpy array to hold the 3D Hand Pose data for each video\n",
        "    dataset = np.zeros((100, 21, 3))\n",
        "\n",
        "    # Loop through each video in the directory\n",
        "    for i, video_file in enumerate(os.listdir(video_dir)):\n",
        "        # Load the video file\n",
        "        cap = cv2.VideoCapture(os.path.join(video_dir, video_file))\n",
        "\n",
        "        # Loop through each frame of the video\n",
        "        while cap.isOpened():\n",
        "            # Read the next frame\n",
        "            ret, image = cap.read()\n",
        "\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Convert the image to RGB and pass it to the MediaPipe Hands model\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            results = hands.process(image)\n",
        "\n",
        "            # Extract the 3D Hand Pose data from the MediaPipe Hands model output\n",
        "            if results.multi_hand_landmarks:\n",
        "                # Loop through each hand landmark in the list\n",
        "                for hand_landmarks in results.multi_hand_landmarks:\n",
        "                    # Get the landmarks for the hand\n",
        "                    landmarks = hand_landmarks.landmark\n",
        "                    # Convert the landmarks to a numpy array\n",
        "                    landmarks_array = np.array([[landmark.x, landmark.y, landmark.z] for landmark in landmarks])\n",
        "                    # Store the landmarks in the dataset array\n",
        "                    dataset[i] = landmarks_array\n",
        "\n",
        "                    # Draw the hand landmarks on the image for visualization\n",
        "                    mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "            # Show the image\n",
        "            #cv2_imshow(image)\n",
        "            #if cv2.waitKey(5) & 0xFF == 27:\n",
        "                #break\n",
        "\n",
        "        # Release the video capture\n",
        "        cap.release()\n",
        "\n",
        "    # Close the windows\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Save the dataset to a numpy file\n",
        "np.save(\"/content/drive/MyDrive/MediaPipe_Data/DatasetNPY_MediaPipe/dataset.npy\", dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpFuYnolDcSO"
      },
      "outputs": [],
      "source": [
        "#Running Successfully\n",
        "\n",
        "# Load the dataset numpy array\n",
        "dataset = np.load(\"/content/drive/MyDrive/MediaPipe_Data/DatasetNPY_MediaPipe/dataset.npy\")\n",
        "\n",
        "# Define the path to save the CSV file\n",
        "csv_path = '/content/drive/MyDrive/MediaPipe_Data/LabelsCSV_MediaPipe/labels.csv'\n",
        "\n",
        "# Define the labels for each video in the dataset\n",
        "labels = [\"angry\"] * 20 + [\"bank\"] * 20 + [\"brother\"] * 20 + [\"bye\"] * 20 + [\"excuse_me\"] * 20\n",
        "\n",
        "# Write the labels to a CSV file\n",
        "with open(csv_path, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"video_id\", \"label\"])\n",
        "    for i, label in enumerate(labels):\n",
        "        writer.writerow([i, label])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTdvvJj3BwPU"
      },
      "outputs": [],
      "source": [
        "# Load dataset and labels\n",
        "# Numpy array of shape (num_videos, num_frames, num_joints, 3) for 3D Hand Pose data\n",
        "# and a list of labels of shape (num_videos,)\n",
        "dataset = np.load(\"dataset.npy\")\n",
        "labels = pd.read_csv(\"labels.csv\")[\"label\"].tolist()\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "num_classes = len(set(labels))\n",
        "one_hot_labels = tf.one_hot(labels, num_classes)\n",
        "\n",
        "# Preprocess the data\n",
        "# Mean normalization and standard deviation scaling to preprocess the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "dataset = scaler.fit_transform(dataset.reshape(-1, dataset.shape[-1])).reshape(dataset.shape)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(dataset, one_hot_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "#Using a simple convolutional neural network\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv3D(32, kernel_size=(3,3,3), activation=\"relu\", input_shape=X_train[0].shape),\n",
        "    tf.keras.layers.MaxPooling3D(pool_size=(2,2,2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
        "\n",
        "#saving the model\n",
        "model.save(\"Tf_mp_model\")\n",
        "\n",
        "# Evaluate the model on a test set\n",
        "# Numpy array of shape (num_videos, num_frames, num_joints, 3) for 3D Hand Pose data and labels for each video\n",
        "test_dataset = np.load(\"test_dataset.npy\")\n",
        "test_labels = pd.read_csv(\"test_labels.csv\")[\"label\"].tolist()\n",
        "test_one_hot_labels = tf.one_hot(test_labels, num_classes)\n",
        "test_dataset = scaler.transform(test_dataset.reshape(-1, test_dataset.shape[-1])).reshape(test_dataset.shape)\n",
        "test_loss, test_acc = model.evaluate(test_dataset, test_one_hot_labels)\n",
        "\n",
        "# Use the trained model to make predictions\n",
        "# Numpy array of shape (num_frames, num_joints, 3) for a single video\n",
        "new_data = np.load(\"new_data.npy\")\n",
        "new_data = scaler.transform(new_data.reshape(-1, new_data.shape[-1])).reshape(1, *new_data.shape)\n",
        "predictions = model.predict(new_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjL0Q3y-RZFx"
      },
      "outputs": [],
      "source": [
        "# Load the trained TensorFlow model\n",
        "model = tf.keras.models.load_model('Tf_mp_model.h5')\n",
        "\n",
        "# Create a MediaPipe Hands object\n",
        "mp_hands = mp.solutions.hands.Hands(max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "\n",
        "# Open the camera\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    # Read a frame from the camera\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        print(\"Failed to read frame from camera\")\n",
        "        break\n",
        "\n",
        "    # Convert the frame to RGB format\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Process the frame with MediaPipe Hands to detect hand landmarks\n",
        "    results = mp_hands.process(frame)\n",
        "\n",
        "    # If hands are detected, extract the landmarks and pass them through the trained model\n",
        "    if results.multi_hand_landmarks:\n",
        "        # Extract the landmarks for the first hand\n",
        "        hand_landmarks = results.multi_hand_landmarks[0]\n",
        "\n",
        "        # Convert the landmarks to a numpy array\n",
        "        landmarks = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])\n",
        "\n",
        "        # Add a batch dimension to the landmarks array\n",
        "        landmarks = np.expand_dims(landmarks, axis=0)\n",
        "\n",
        "        # Pass the landmarks through the trained model to predict the sign language word being signed\n",
        "        prediction = model.predict(landmarks)\n",
        "        predicted_word = np.argmax(prediction)\n",
        "\n",
        "        # Draw the predicted word on the frame\n",
        "        cv2.putText(frame, \"Predicted word: {}\".format(predicted_word), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "    # Convert the frame back to BGR format for display\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Display the frame\n",
        "    cv2.imshow(\"Sinhala Sign Language Recognition: \", frame)\n",
        "\n",
        "    # Exit if the user presses the 'q' key\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the camera and destroy the display window\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOOJVAKPVYMw"
      },
      "outputs": [],
      "source": [
        "# Load the test dataset\n",
        "test_data = np.load('test_dataset.npy')\n",
        "test_labels = np.loadtxt('test_labels.csv', delimiter=',')\n",
        "\n",
        "# Load the saved model\n",
        "model = tf.keras.models.load_model('Tf_mp_model')\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
        "\n",
        "# Make predictions on the test dataset\n",
        "test_predictions = np.argmax(model.predict(test_data), axis=1)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(test_labels, test_predictions)\n",
        "\n",
        "# Compute the F1 score\n",
        "f1 = f1_score(test_labels, test_predictions, average='macro')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)\n",
        "print('Confusion Matrix:\\n', cm)\n",
        "print('F1 Score:', f1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "13koHaRWEcuouQPn-DEapCJICkzA8hILU",
      "authorship_tag": "ABX9TyOCUwknzzAvVYWcnD3sEWKx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}